{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Chatbot Xeno.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3_T4zUgaPeS",
        "outputId": "29cfec46-8d68-411e-dacb-8c8f7d7843a2"
      },
      "source": [
        "!pip install tflearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[?25l\r\u001b[K     |███                             | 10 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 20 kB 21.8 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 30 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 40 kB 17.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 51 kB 14.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 61 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 71 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 81 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 92 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 102 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 107 kB 13.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=b38a42f72b982889d8b35a279fe8eb619121d4ef1f119c2e8f30fd9e96e349d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/14/2e/1d8e28cc47a5a931a2fb82438c9e37ef9246cc6a3774520271\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag7Rj9Q7aNgG",
        "outputId": "08718714-ca31-46c8-88b0-48210f0a5080"
      },
      "source": [
        "#importing all the necessary packages and libraries \n",
        "import nltk\n",
        "import pandas as pd\n",
        "import random as ran\n",
        "import string\n",
        "import numpy\n",
        "import tflearn\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import pickle\n",
        "from tabulate import tabulate\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from random import randrange\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#downloading all the necessary data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "stop = stopwords.words('english')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#defining all the methods\n",
        "\n",
        "#method for trainig the chatbot\n",
        "def createmodel(train, out_layer):\n",
        "  tf.compat.v1.reset_default_graph()\n",
        "  #taking the train's first string shape for the input layer and defining other\n",
        "  #training layers as well\n",
        "  neural_layer = tflearn.input_data(shape=[None, len(train[0])])\n",
        "  neural_layer = tflearn.fully_connected(neural_layer, 16)\n",
        "  neural_layer = tflearn.fully_connected(neural_layer, 8)\n",
        "  neural_layer = tflearn.fully_connected(neural_layer, 8)\n",
        "  neural_layer = tflearn.fully_connected(neural_layer, len(out_layer[0]), \n",
        "                                         activation=\"softmax\")\n",
        "  #using softmax as the activation function\n",
        "  neural_layer = tflearn.regression(neural_layer)\n",
        "  return neural_layer\n",
        "\n",
        "#method for playing atlas game\n",
        "def atlas(count):\n",
        "  #reading the dataset and converting it to list\n",
        "  #Dataset for countries\n",
        "  #https://textlists.info/geography/countries-of-the-world/\n",
        "  #Dataset for cities\n",
        "  #https://simplemaps.com/data/world-cities\n",
        "  l = pd.read_excel('Atlas.xlsx', sheet_name=0)\n",
        "  atlas = l['NAMES'].tolist()\n",
        "  limit = count\n",
        "  #sorting it and lowercasing the names\n",
        "  sort_A = sorted(atlas)\n",
        "  a = [i.lower() for i in sort_A]\n",
        "\n",
        "  c = 0\n",
        "  #choosing a random alphabet\n",
        "  ran_letter = ran.choice(string.ascii_letters)\n",
        "  response_lw = ran_letter\n",
        "  response_lw = response_lw.lower()\n",
        "  while(limit>0):\n",
        "      \n",
        "      \n",
        "      b = True\n",
        "      \n",
        "      \n",
        "      if c == 0:\n",
        "         \n",
        "          print(\"Xeno: So lets start the game of Atlas with a random letter....You go first with the letter\", ran_letter)\n",
        "    \n",
        "      while (b):\n",
        "        #waiting for user to enter name of the city/country\n",
        "          x = input().lower()\n",
        "          \n",
        "          #removing the name if its in the dataset and has been used\n",
        "          if x in a and x[0] == response_lw:\n",
        "              a.remove(x)\n",
        "              b = False\n",
        "          \n",
        "          else:\n",
        "              print('Xeno: Try another Name')\n",
        "              b = True\n",
        "              \n",
        "      y = x[-1]\n",
        "      #excluding q and x for starting words at the first round.\n",
        "      notwanted = 'qx'\n",
        "      r = [j for j in a if j.lower().startswith(y) if j not in notwanted] \n",
        "      \n",
        "      #random choice for Xeno, so it can choose a name \n",
        "      response = ran.choice(r)\n",
        "      response_lw = response[-1]\n",
        "      response_lw = response_lw.lower()\n",
        "      print(\"Xeno:\",response)\n",
        "      a.remove(response)\n",
        "      c+=1\n",
        "      limit = limit - 1\n",
        "#method for bag of words for one hot encoding\n",
        "def bag_of_words(t, words):\n",
        "#initially making the bag list 0\n",
        "  bag = []\n",
        "  for i in range(len(words)):\n",
        "    bag.append(0)\n",
        "\n",
        "  s_words = nltk.word_tokenize(t)\n",
        "  s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
        "\n",
        "  #if word is present then we add 1 to that index\n",
        "  for se in s_words:\n",
        "      for i, w in enumerate(words):\n",
        "          if w == se:\n",
        "              bag[i] = 1\n",
        "\n",
        "  return numpy.array(bag)\n",
        "\n",
        "# method for name extraction from string \n",
        "def nameextraction(customer_input):\n",
        "    customer_name = []\n",
        "    #splitting the string and after tokenisation, using POS to tag the words \n",
        "    customer_input = ' '.join([i for i in customer_input.split() if i not in \n",
        "                               stop])\n",
        "    sentence = nltk.sent_tokenize(customer_input)\n",
        "    sentence = [nltk.word_tokenize(sent) for sent in sentence]\n",
        "    sentence = [nltk.pos_tag(sent) for sent in sentence]\n",
        "    #checking the tags assigned if its Person or not\n",
        "    for tagged_sentence in sentence:\n",
        "        for chunk in nltk.ne_chunk(tagged_sentence):\n",
        "            if type(chunk) == nltk.tree.Tree:\n",
        "                if chunk.label() == 'PERSON': \n",
        "                  #if person then adding it to separate list\n",
        "                    customer_name.append(' '.join([c[0] for c in chunk]))\n",
        "    return customer_name\n",
        "\n",
        "# method for splitting the list by spaces to get the first name (not firstname)\n",
        "def namesplit(name):\n",
        "    new_name = []\n",
        "    for i in name:\n",
        "      new_name.extend(i.split())  \n",
        "    return new_name \n",
        "\n",
        "## Defining training parameters\n",
        "epoch = 1000\n",
        "b_size = 8\n",
        "metric = True\n",
        "##Stemmer initialised and language selected\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "with open(\"intent.json\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "try:\n",
        "    with open(\"data.pickle\", \"rb\") as f:\n",
        "        words, labels, training, output = pickle.load(f)\n",
        "except:\n",
        "    words = []\n",
        "    labels = []\n",
        "    a_doc = []\n",
        "    b_doc = []\n",
        "    #going through the intent file and storing the data\n",
        "    for intent in data[\"intent\"]:\n",
        "        for pattern in intent[\"patterns\"]:\n",
        "            wrds = nltk.word_tokenize(pattern)\n",
        "            words.extend(wrds)\n",
        "            a_doc.append(wrds)\n",
        "            b_doc.append(intent[\"tag\"])\n",
        "\n",
        "        if intent[\"tag\"] not in labels:\n",
        "            labels.append(intent[\"tag\"])\n",
        "\n",
        "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
        "    words = sorted(list(set(words)))\n",
        "\n",
        "    labels = sorted(labels)\n",
        "\n",
        "    training = []\n",
        "    output = []\n",
        "\n",
        "    out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "    \n",
        "    for x, doc in enumerate(a_doc):\n",
        "        bag = []\n",
        "\n",
        "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
        "        #simple one hot encoding\n",
        "        for w in words:\n",
        "            if w in wrds:\n",
        "                bag.append(1)\n",
        "            else:\n",
        "                bag.append(0)\n",
        "\n",
        "        output_row = out_empty[:]\n",
        "        output_row[labels.index(b_doc[x])] = 1\n",
        "\n",
        "        training.append(bag)\n",
        "        output.append(output_row)\n",
        "\n",
        "\n",
        "    training = numpy.array(training)\n",
        "    output = numpy.array(output)\n",
        "\n",
        "    with open(\"data.pickle\", \"wb\") as f:\n",
        "        pickle.dump((words, labels, training, output), f)\n",
        "#model training\n",
        "layer = createmodel(training, output)\n",
        "model = tflearn.DNN(layer)\n",
        "model.fit(training, output, n_epoch=epoch, batch_size = b_size, \n",
        "          show_metric=metric)\n",
        "model.save(\"model.tflearn\")\n",
        "\n",
        "#Converting the Menu in List to tablular format for better display\n",
        "table = [['Item No', 'Name', 'Price(in GBP)'], ['1', 'Chicken Burger Meal', 8], \n",
        "         ['2', 'Vegan Burger Meal', 7], ['3', 'Mac n Cheese', 7], \n",
        "         ['4', 'Chicken Wrap', 6], ['5', 'Veggie Wrap', 5]]\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 8999  | total loss: \u001b[1m\u001b[32m0.00015\u001b[0m\u001b[0m | time: 0.059s\n",
            "| Adam | epoch: 1000 | loss: 0.00015 - acc: 1.0000 -- iter: 64/69\n",
            "Training Step: 9000  | total loss: \u001b[1m\u001b[32m0.00015\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 1000 | loss: 0.00015 - acc: 1.0000 -- iter: 69/69\n",
            "--\n",
            "INFO:tensorflow:/content/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzWhoqJ-CG1j",
        "outputId": "814b3caa-5491-4c4c-d832-58f04f9be383"
      },
      "source": [
        "#main- It is asking for user name, using the methods defined above and matching the intent tags\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Welcome to the Food Station!\") \n",
        "    print(\"Tell me your name so we can start chatting!\") \n",
        "    print(\"After ordering type Quit to exit\")\n",
        "    text = input(\"You: \")\n",
        "    n_s = []\n",
        "    if len(text) == 1:\n",
        "      n_s.append(text)\n",
        "    else:\n",
        "      name = nameextraction(text)\n",
        "      n_s = namesplit(name)\n",
        "    #fixing the name bias since all Indian names cant be extracted \n",
        "    #(Eg: Kushagra is not extracted as a name)\n",
        "    if not n_s:\n",
        "      n_s.append(\"Customer\")\n",
        "    print(\"Hello\", n_s[0], \"To start chatting with Xeno\")\n",
        "    print(\"You can:\")\n",
        "    print(\"Greet the chat bot to start the conversation\")\n",
        "    \n",
        "    flag = True\n",
        "    while flag: \n",
        "      if flag == True:\n",
        "        inp = input(\"You: \") \n",
        "      if inp.lower() == \"quit\": \n",
        "        break\n",
        "      if inp.lower() == \"atlas\":\n",
        "        break\n",
        "      if inp.isdigit() and len(inp) == 5:\n",
        "        print(\"Xeno: Great!\")\n",
        "        break\n",
        "      #predicting which word belongs to which tag from the intent file\n",
        "      results = model.predict([bag_of_words(inp, words)])\n",
        "      results_index = numpy.argmax(results)\n",
        "      tag = labels[results_index]\n",
        "      #if tag is equal to a certain category, then the response from that section \n",
        "      #is sent.       \n",
        "      for tg in data[\"intent\"]:\n",
        "          if tg['tag'] == tag:\n",
        "              responses = tg['responses']\n",
        "              if tag == \"greeting\":\n",
        "                print(\"Xeno:\",n_s[0], ran.choice(responses))\n",
        "                print(\">The menu\")\n",
        "                print(\">Our Timings\")\n",
        "                print(\">About me\")\n",
        "                break\n",
        "    \n",
        "              if tag == 'Food':\n",
        "                print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))\n",
        "                print('Xeno: Please Enter the order No. Due to covid, we have limited options')\n",
        "                #opt = input()\n",
        "                #print(random.choice(responses))\n",
        "    \n",
        "              if tag == 'ordering':\n",
        "                print(\"Order\",ran.choice(responses))\n",
        "                u_id = input()\n",
        "                       \n",
        "    \n",
        "              if tag == 'no id':\n",
        "                print(ran.choice(responses))\n",
        "                print(\"Xeno:Your new Customer ID is\",randrange(10000, 99999))\n",
        "                flag = False\n",
        "            \n",
        "              else:\n",
        "                print(ran.choice(responses))\n",
        "\n",
        "    print(\"Xeno: While you are waiting for your order, you can play a game of ATLAS with me.\")\n",
        "    print(\"Xeno: Please type yes or no....I am good at it :)\")\n",
        "    u = input()\n",
        "    l = len(u)\n",
        "    count = ran.randrange(5,8)\n",
        "    if u[-1] == '!' or '.':\n",
        "     u = u[:l-1]\n",
        "    #asking if user wants to play atlas.\n",
        "    if u.lower() == 'yes':\n",
        "      print(\"Rules are simple: Name a city or country starting with the end letter of the previous name.\")\n",
        "      print(\"you can say each name only once.\")\n",
        "      atlas(count)\n",
        "    else: \n",
        "      (\"You can wait for your order. We are working on it!\")\n",
        "          \n",
        "    print(\"Xeno: Your order will be ready shortly. Thanks for choosing Food Station!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Food Station!\n",
            "Tell me your name so we can start chatting!\n",
            "After ordering type Quit to exit\n",
            "You: Holly\n",
            "Hello Customer To start chatting with Xeno\n",
            "You can:\n",
            "Greet the chat bot to start the conversation\n",
            "You: hi\n",
            "Xeno: Customer Hello! Ask me about\n",
            ">The menu\n",
            ">Our Timings\n",
            ">About me\n",
            "You: 2\n",
            "Order  on the way. Your Order will be ready in 2 minutes. You can tell me your Customer Id if you have one. Just type your 5 digit no. If Not then I can assign one to you.\n",
            "12345\n",
            " on the way. Your Order will be ready in 2 minutes. You can tell me your Customer Id if you have one. Just type your 5 digit no. If Not then I can assign one to you.\n",
            "You: 1324\n",
            "Order  on the way. Your Order will be ready in 2 minutes. You can tell me your Customer Id if you have one. Just type your 5 digit no. If Not then I can assign one to you.\n",
            "12345\n",
            " on the way. Your Order will be ready in 2 minutes. You can tell me your Customer Id if you have one. Just type your 5 digit no. If Not then I can assign one to you.\n",
            "You: 12345\n",
            "Xeno: Great!\n",
            "Xeno: While you are waiting for your order, you can play a game of ATLAS with me.\n",
            "Xeno: Please type yes or no....I am good at it :)\n",
            "no\n",
            "Xeno: Your Order is ready. Thanks for choosing Food Station!\n"
          ]
        }
      ]
    }
  ]
}